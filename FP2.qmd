---
title: "FP2"
format: html
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(Hmisc)

mdb.get('XXH2023_YRBS_Data.mdb', tables = TRUE)
data <- mdb.get('XXH2023_YRBS_Data.mdb', tables = "XXHq")
subset <- data |>
  select(q1:q107) |> 
  select(!q6:q7)|>
  select_if(is.numeric)

data_clean <- na.omit(subset)
```

# PCA
```{r}
# Compute PCs
pr.out <- prcomp(data_clean, scale = TRUE)
pr.out$rotation # rotation matrices for each PC 

# PC1 v PC2 plot
biplot(pr.out, scale = 0)

# evaluate variance of PCs
pr.out$sdev # standard dev.
pr.var <- pr.out$sdev^2 # varience
pr.var
pve <- pr.var / sum(pr.var) # proportion varience explained
pve

head(pve*100) # top percentages of varience explained (first 6 PCs)
```

```{r}
par(mfrow = c(1, 2))
# Scree plot of subset
  # only first 10 components are visualized to better determine the location of the plots "elbow" 
plot(pve[1:10], xlab = "Principal Component",
     ylab = "Proportion of Variance Explained", ylim = c(0,1),
     type = "b")

# Cumulative explained variance
  # First 10 PCs to match visualization above
plot(cumsum(pve[1:10]), xlab = "Principal Component", ylab = "Cumulative Proprotion of Variance Explained", ylim = c(0,1), type = "b")

# Cumulative proportion variance explained by first 3 components
sum((pve[1:3]))
```


```{r}
# matrix multiplication 
pc_space <- as.matrix(data_clean) %*% pr.out$rotation[,1:3]
```

# Next steps
Now that we have calculated our Principal Component Analysis, our next step in our plan of action is to pull out ~4 PCs from that capture the most variance. Then, we will do some data wrangling to obtain a PC score vector for each observation in the dataset. After we have completed this step, we will either use hierarchical clustering or k-means clustering in order to cluster our data into groups. We will label each observation into a specific cluster. Lastly, we will do discriminant analysis in order to pull out which specific predictors are the most important in separating the clusters from one another. 

# K-means clustering
```{r}
set.seed(1)

km.out <- kmeans(data_clean, 4, nstart = 20)
km.out
```

# Hierarchical clustering

# Discriminant Analysis:
## Linear Discriminant Analysis (LDA):

To conduct LDA we first must assume that the data is normally distributed and that each of the clusters have equal variance. We're assuming that the data meets each of the requirements, even though it may not be true.

```{r}
# LDA using training data 
lda.cluster <- lda(cluster ~ PC1 + PC2 + PC3, data = data_clean.train)
lda.cluster

plot(lda.cluster)
```

Most of the observations in the survey belong to the 1 cluster, specifically 1a. 68.7% of the surveys correspond to the 1a cluster, 20.4% to 1b, and 9.6% to 1c. For cluster 1, 98.7% of the observations belong in that group, which indicates that grouping are unfairly distributed.

```{r}
lda.pred <- predict(ld.cluster, data_clean.test)
lda.pred <- lda.pred$class

# confustion matrix
confusion_matrix <- table(lda.pred, data_clean.test$cluster)
print(confusion_matrix)

# accuracy of matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# overall error rate
1-accuracy

# class-specific error rates
one.a <- 1 - confusion_matrix[1,1]/(sum(confusion_matrix[1, ]))
one.b <- 1 - confusion_matrix[2, 2]/(sum(confusion_matrix[2, ]))
one.c <- 1 - confusion_matrix[3, 3]/(sum(confusion_matrix[3, ]))
two <- 1 - confusion_matrix[4, 4]/(sum(confusion_matrix[4, ]))
three <- 1 - confusion_matrix[5, 5]/(sum(confusion_matrix[5, ]))

label <- c("1a:", "1b:", "1c:", "2:", "3:")
error.rates <- c(round(one.a, 4), round(one.b, 4), round(one.c, 4), round(two, 4), round(three, 4))
lda.spec <- tibble(label, error.rates)
lda.spec
```

The overall accuracy rate is 95.3% while the overall test error rate is 4.7%. This is a really low test error rate. The cluster specific error rates are relatively low as well. Cluster 2 has the highest error rate of 38.98% while cluster 3 has a 0% error rate.

## Naive Bayes:

```{r}
library(naivebayes)
nb.cluster <- naive_bayes(cluster ~ PC1 + PC2 + PC3, data = data_clean.train)
nb.cluster

nb.pred <- predict(nb.cluster, data_clean.test)


confusion_matrix <- table(nb.pred, data_clean.test$cluster)
print(confusion_matrix)

# accuracy of matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# overall error rate
1-accuracy

# class-specific error rates
one.a <- 1 - confusion_matrix[1,1]/(sum(confusion_matrix[1, ]))
one.b <- 1 - confusion_matrix[2, 2]/(sum(confusion_matrix[2, ]))
one.c <- 1 - confusion_matrix[3, 3]/(sum(confusion_matrix[3, ]))
two <- 1 - confusion_matrix[4, 4]/(sum(confusion_matrix[4, ]))
three <- 1 - confusion_matrix[5, 5]/(sum(confusion_matrix[5, ]))

error.rates <- c(round(one.a, 4), round(one.b, 4), round(one.c, 4), round(two, 4), round(three, 4))
nb.spec <- tibble(label, error.rates)
nb.spec
```

The overall accuracy rate is 94.8% which is slightly lower than accuracy rates of QDA and LDA. The overall test error is 5.2%, which is a bit more than both of the previous methods.

