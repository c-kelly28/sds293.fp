---
title: "fp2_copy"
format: html
editor_options: 
  chunk_output_type: console
---

---
title: "FP2"
format: html
editor_options: 
  chunk_output_type: console
---

subset \<- data %\>%

select(q1:q107)

```{r}
library(tidyverse)
library(Hmisc)

mdb.get('XXH2023_YRBS_Data.mdb', tables = TRUE)
data <- mdb.get('XXH2023_YRBS_Data.mdb', tables = "XXHq")

subset <- data |>
  dplyr::select(q1:q107) |> 
  dplyr::select(!q6:q7)|>
  dplyr::select_if(is.numeric)

data_clean <- na.omit(subset)
```

# PCA

```{r}
# Compute PCs
pr.out <- prcomp(data_clean, scale = TRUE)
pr.out$rotation # rotation matrices for each PC 

# PC1 v PC2 plot
biplot(pr.out, scale = 0)

# evaluate variance of PCs
pr.out$sdev # standard dev.
pr.var <- pr.out$sdev^2 # varience
pr.var
pve <- pr.var / sum(pr.var) # proportion varience explained
pve

head(pve*100) # top percentages of varience explained (first 6 PCs)
```

```{r}
par(mfrow = c(1, 2))
# Scree plot of subset
  # only first 10 components are visualized to better determine the location of the plots "elbow" 
plot(pve[1:10], xlab = "Principal Component",
     ylab = "Proportion of Variance Explained", ylim = c(0,1),
     type = "b")

# Cumulative explained variance
  # First 10 PCs to match visualization above
plot(cumsum(pve[1:10]), xlab = "Principal Component", ylab = "Cumulative Proprotion of Variance Explained", ylim = c(0,1), type = "b")

# Cumulative proportion variance explained by first 3 components
sum((pve[1:3]))
```

```{r}
pc_space <- as.matrix(data_clean) %*% pr.out$rotation[,1:3]

pc_space <- as.data.frame(pc_space)

data_clean <- cbind(data_clean, pc_space)
```

# Next steps

Now that we have calculated our Principal Component Analysis, our next step in our plan of action is to pull out \~4 PCs from that capture the most variance. Then, we will do some data wrangling to obtain a PC score vector for each observation in the dataset. After we have completed this step, we will either use hierarchical clustering or k-means clustering in order to cluster our data into groups. We will label each observation into a specific cluster. Lastly, we will do discriminant analysis in order to pull out which specific predictors are the most important in separating the clusters from one another.

# K-means clustering

```{r}
set.seed(1)

km.out <- kmeans(data_clean, 4, nstart = 20)
km.out
```

```{r}
library(ISLR2)
nci.labs <- NCI60$labs
nci.data <- NCI60$labs
```

# Hierarchical clustering

```{r}
# graphing function
Cols <- function(vec) {
  cols <- rainbow(length(unique(vec)))
 return(cols[as.numeric(as.factor(vec))])
  }
```

```{r}
par(mfrow = c(1, 2))
plot(data_clean[105:106], pch = 19,
xlab = "PC1", ylab = "PC2")

plot(data_clean[c(105, 107)], pch = 19,
xlab = "PC1", ylab = "PC3")

plot(data_clean[c(106, 107)], pch = 19,
xlab = "PC2", ylab = "PC3")
```

```{r}
summary(data_clean[105:107])
plot(data_clean[105:107])
```

```{r}
par(mfrow = c(1, 3))
```

```{r}
#plot 3 denodrograms
par(mfrow = c(1, 1))
data.dist <- dist(pc_space)

# plot(hclust(data.dist), xlab = "", sub = "", ylab = "", main = "Complete Linkage")

plot(hclust(data.dist , method = "average"), main = "Average Linkage", labels = FALSE,
xlab = "", sub = "", ylab = "")

#plot(hclust(data.dist , method = "single"), main = "Single Linkage", xlab = "", sub = "", ylab = "")
```

```{r}
hc.out <- hclust(dist(pc_space), method = "average")
hc.clusters <- cutree(hc.out , 3)

data_clean <- cbind(data_clean, as.data.frame(hc.clusters))
```

```{r}
plot(pc_space[, 1:2], col = Cols(hc.clusters), pch = 19)
text(pc_space[, 1:2], labels = hc.clusters, col = Cols(hc.clusters), pos = 3)
```

```{r}
par(mfrow = c(1, 3)) 
 # need to change the data set
plot(pr.out$x[, 1:2], col = Cols(hc.clusters), pch = 19,
     xlab = "PC1", ylab = "PC2", main = "PC1 vs PC2")

plot(pr.out$x[, c(1, 3)], col = Cols(hc.clusters), pch = 19,
     xlab = "PC1", ylab = "PC3", main = "PC1 vs PC3")

plot(pr.out$x[, 2:3], col = Cols(hc.clusters), pch = 19,
     xlab = "PC2", ylab = "PC3", main = "PC2 vs PC3")
```

```{r}
plotly::plot_ly(data_clean, x = ~PC1, y = ~PC2, z = ~PC3, color = hc.clusters)
```

Upon examination of the above 3D plot, it became clear that there were additional clusters within what was being classified as "cluster 1". However, these additional clusters were not captured after changing the number of clusters requested. This is because of the skewed nature of the data and use of Euclidean distance. The skewed nature of the data means that the more extreme values, which are further apart in Euclidean space, will be separate with the addition of new clusters. However, we can see that the larger cluster 1, is in fact composed of multiple clusters. To capture this appropriately, the extreme clusters identified in this hierarchical clustering was separated from the data. A second PCA and hierarchical clustering was conducted on what was considered group "1." This analysis identified three clusters, resulting in a total of 5 clusters identified to be evaluted using discriminant analysis.

```{r}
subset <- filter(data_clean, hc.clusters == 1)
subset <- subset[,1:105]
subset <- rename(subset, ID = PC1)
```

```{r}
# Compute PCs
pr.out2 <- prcomp(subset[, 1:104], scale = TRUE)
pr.out2$rotation # rotation matrices for each PC 

# PC1 v PC2 plot
biplot(pr.out2, scale = 0)

# evaluate variance of PCs
pr.out2$sdev # standard dev.
pr.var2 <- pr.out2$sdev^2 # varience
pr.var2
pve2 <- pr.var2 / sum(pr.var2) # proportion varience explained
pve2

head(pve2*100) # top percentages of varience explained (first 6 PCs)
```

```{r}
par(mfrow = c(1, 2))
# Scree plot of subset
  # only first 10 components are visualized to better determine the location of the plots "elbow" 
plot(pve2[1:10], xlab = "Principal Component",
     ylab = "Proportion of Variance Explained", ylim = c(0,1),
     type = "b")

# Cumulative explained variance
  # First 10 PCs to match visualization above
plot(cumsum(pve2[1:10]), xlab = "Principal Component", ylab = "Cumulative Proprotion of Variance Explained", ylim = c(0,1), type = "b")

# Cumulative proportion variance explained by first 3 components
sum((pve2[1:3]))
```

```{r}

pc_space2 <- as.matrix(subset[1:104]) %*% pr.out2$rotation[,1:3]

pc_space2 <- as.data.frame(pc_space2)

subset <- cbind(subset, pc_space2)
```

```{r}
par(mfrow = c(1, 2))
plot(subset[106:107], pch = 19,
xlab = "PC1", ylab = "PC2")

plot(subset[c(106, 108)], pch = 19,
xlab = "PC1", ylab = "PC3")

plot(subset[c(107, 108)], pch = 19,
xlab = "PC2", ylab = "PC3")
```

```{r}
summary(subset[106:108])
plot(subset[106:108])
```

```{r}
par(mfrow = c(1, 3))
```

```{r}
#plot 3 denodrograms
par(mfrow = c(1, 1))
data.dist2 <- dist(pc_space2)

# plot(hclust(data.dist), xlab = "", sub = "", ylab = "", main = "Complete Linkage")

plot(hclust(data.dist2 , method = "average"), main = "Average Linkage",
xlab = "", sub = "", ylab = "")

#plot(hclust(data.dist , method = "single"), main = "Single Linkage", xlab = "", sub = "", ylab = "")
```

```{r}
hc.out2 <- hclust(dist(pc_space2), method = "average")
hc.clusters2 <- cutree(hc.out2 , 3)

subset <- cbind(subset, as.data.frame(hc.clusters2))
```

```{r}
plotly::plot_ly(subset, x = ~PC1, y = ~PC2, z = ~PC3, color = hc.clusters2)
```

```{r}
subset$hc.clusters2 <- recode(hc.clusters2, `1` = "1a", `2` = "1b", `3` = "1c")

subset_clusters <- subset[, c(105,109)]
```

```{r}
data_clean <- left_join(data_clean, subset_clusters, by = join_by(PC1 == ID))

data_clean$hc.clusters <- as.character(data_clean$hc.clusters) 

cluster <- rep.int(NA, 7242)

for (i in 1:7242) {
  if (data_clean$hc.clusters[i] == "1") {
    cluster[i] <- data_clean$hc.clusters2[i]
  } else {
    cluster[i] <- data_clean$hc.clusters[i]
  }
}

data_clean <- cbind(data_clean, as.data.frame(cluster))
```

```{r}
plotly::plot_ly(data_clean, x = ~PC1, y = ~PC2, z = ~PC3, color = cluster, size = 8)
```

# Discriminant Analysis:

## Linear Discriminant Analysis (LDA):

To conduct LDA we first must assume that the data is normally distributed and that each of the clusters have equal variance. We're assuming that the data meets each of the requirements, even though it may not be true.

```{r}
# separate the data into training and testing 
set.seed(1)
train <- sample(1:nrow(data_clean), size = 3621)
test <- (-train)
data_clean.train <- data_clean[train, ] # training data
data_clean.test <- data_clean[-train, ] # test data
```

```{r}
library(ISLR2)
library(MASS)

# LDA using training data 
lda.cluster <- lda(cluster ~ PC1 + PC2 + PC3, data = data_clean.train)
lda.cluster

plot(lda.cluster)
```

Most of the observations in the survey belong to the 1 cluster, specifically 1a. 68.7% of the surveys correspond to the 1a cluster, 20.4% to 1b, and 9.6% to 1c. For cluster 1, 98.7% of the observations belong in that group, which indicates that grouping are unfairly distributed.

```{r}
lda.pred <- predict(ld.cluster, data_clean.test)
lda.pred <- lda.pred$class

# confustion matrix
confusion_matrix <- table(lda.pred, data_clean.test$cluster)
print(confusion_matrix)

# accuracy of matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# overall error rate
1-accuracy

# class-specific error rates
one.a <- 1 - confusion_matrix[1,1]/(sum(confusion_matrix[1, ]))
one.b <- 1 - confusion_matrix[2, 2]/(sum(confusion_matrix[2, ]))
one.c <- 1 - confusion_matrix[3, 3]/(sum(confusion_matrix[3, ]))
two <- 1 - confusion_matrix[4, 4]/(sum(confusion_matrix[4, ]))
three <- 1 - confusion_matrix[5, 5]/(sum(confusion_matrix[5, ]))

paste("1a:" ,round(one.a, 4))
paste("1b:" ,round(one.b, 4))
paste("1c:" ,round(one.c, 4))
paste("2:" ,round(two, 4))
paste("3:" ,round(three, 4))
```

The overall accuracy rate is 95.3% while the overall test error rate is 4.7%. This is a really low test error rate. The cluster specific error rates are relatively low as well. Cluster 2 has the highest error rate of 38.98% while cluster 3 has a 0% error rate.

## Quadratic Discriminant Analysis (QDA):

The assumptions for QDA are more relaxed in that we don't need to have a common variance within the clusters. This is a much more realistic viewing of the data since the clusters are so uneven. However, in choosing this method we will have higher variance and lower bias. So, we're assuming that the data has low variance and high bias.

```{r}
qda.cluster <- qda(cluster ~ PC1 + PC2 + PC3, data = data_clean.train)
qda.cluster

qda.pred <- predict(qda.cluster, data_clean.test)
qda.pred <- qda.pred$class

confusion_matrix <- table(qda.pred, data_clean.test$cluster)
print(confusion_matrix)

# accuracy of matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# overall error rate
1-accuracy

# class-specific error rates
one.a <- 1 - confusion_matrix[1,1]/(sum(confusion_matrix[1, ]))
one.b <- 1 - confusion_matrix[2, 2]/(sum(confusion_matrix[2, ]))
one.c <- 1 - confusion_matrix[3, 3]/(sum(confusion_matrix[3, ]))
two <- 1 - confusion_matrix[4, 4]/(sum(confusion_matrix[4, ]))
three <- 1 - confusion_matrix[5, 5]/(sum(confusion_matrix[5, ]))

paste("1a:" ,round(one.a, 4))
paste("1b:" ,round(one.b, 4))
paste("1c:" ,round(one.c, 4))
paste("2:" ,round(two, 4))
paste("3:" ,round(three, 4))
```

The overall accuracy rate is 95.2% which is slightly lower than LDA's but so minutely that it doesn't matter too much. The overall test error is 4.8%.

## Naive Bayes:

```{r}
library(naivebayes)
nb.cluster <- naive_bayes(cluster ~ PC1 + PC2 + PC3, data = data_clean.train)
nb.cluster

nb.pred <- predict(nb.cluster, data_clean.test)


confusion_matrix <- table(nb.pred, data_clean.test$cluster)
print(confusion_matrix)

# accuracy of matrix
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# overall error rate
1-accuracy

# class-specific error rates
one.a <- 1 - confusion_matrix[1,1]/(sum(confusion_matrix[1, ]))
one.b <- 1 - confusion_matrix[2, 2]/(sum(confusion_matrix[2, ]))
one.c <- 1 - confusion_matrix[3, 3]/(sum(confusion_matrix[3, ]))
two <- 1 - confusion_matrix[4, 4]/(sum(confusion_matrix[4, ]))
three <- 1 - confusion_matrix[5, 5]/(sum(confusion_matrix[5, ]))

paste("1a:" ,round(one.a, 4))
paste("1b:" ,round(one.b, 4))
paste("1c:" ,round(one.c, 4))
paste("2:" ,round(two, 4))
paste("3:" ,round(three, 4))
```

The overall accuracy rate is 94.8% which is slightly lower than accuracy rates of QDA and LDA. The overall test error is 5.2%, which is a bit more than both of the previous methods.
